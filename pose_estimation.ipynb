{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLxUgrvQGg-T"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow opencv-python ultralytics mediapipe==0.10.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "1ntl1jOTGmZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mediapipe"
      ],
      "metadata": {
        "id": "k9g3Dzkp-Vgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "def plot_3d_landmarks(landmarks):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    x_vals = [l.x for l in landmarks]\n",
        "    y_vals = [l.y for l in landmarks]\n",
        "    z_vals = [l.z for l in landmarks]\n",
        "    ax.scatter(x_vals, y_vals, z_vals, c='r')\n",
        "\n",
        "    # Draw connections like POSE_CONNECTIONS if desired\n",
        "    for connection in mp_pose.POSE_CONNECTIONS:\n",
        "        start_idx = connection[0].value\n",
        "        end_idx = connection[1].value\n",
        "        ax.plot([x_vals[start_idx], x_vals[end_idx]],\n",
        "                [y_vals[start_idx], y_vals[end_idx]],\n",
        "                [z_vals[start_idx], z_vals[end_idx]], c='b')\n",
        "\n",
        "    ax.view_init(elev=10, azim=10)  # Customize angle\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "uPZPywHrMOkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "\n",
        "mp_pose = mp.solutions.pose\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "pose = mp_pose.Pose()\n",
        "\n",
        "cap = cv2.VideoCapture(\"/content/drive/MyDrive/smai_project/data_set/1.mp4\")\n",
        "out = cv2.VideoWriter(\"/content/drive/MyDrive/smai_project/output/1_mp.mp4\",\n",
        "                      cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "                      cap.get(cv2.CAP_PROP_FPS),\n",
        "                      (int(cap.get(3)), int(cap.get(4))))\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret: break\n",
        "\n",
        "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    res = pose.process(rgb)\n",
        "\n",
        "    if res.pose_landmarks:\n",
        "        mp_drawing.draw_landmarks(frame, res.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "pose.close()\n"
      ],
      "metadata": {
        "id": "lAVHT__JHCp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Movenet"
      ],
      "metadata": {
        "id": "sxYI9fua-YFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Load the MoveNet MultiPose model\n",
        "model = hub.load(\"https://tfhub.dev/google/movenet/multipose/lightning/1\")\n",
        "\n",
        "# Set up video I/O\n",
        "cap = cv2.VideoCapture(\"/content/drive/MyDrive/smai_project/data_set/1.mp4\")\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "out = cv2.VideoWriter(\n",
        "    \"/content/drive/MyDrive/smai_project/output/1_mn.mp4\",\n",
        "    cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "    fps,\n",
        "    (width, height)\n",
        ")\n",
        "\n",
        "# Standard MoveNet 17-keypoint skeleton edges\n",
        "SKELETON = [\n",
        "    (0, 1), (0, 2),\n",
        "    (1, 3), (2, 4),\n",
        "    (0, 5), (0, 6),\n",
        "    (5, 7), (7, 9),\n",
        "    (6, 8), (8, 10),\n",
        "    (5, 6), (5, 11),\n",
        "    (6, 12), (11, 12),\n",
        "    (11, 13), (13, 15),\n",
        "    (12, 14), (14, 16)\n",
        "]\n",
        "\n",
        "def draw_keypoints_and_skeleton(frame, keypoints, conf_thresh=0.3):\n",
        "    \"\"\"\n",
        "    Draw keypoints and skeleton on the frame.\n",
        "    keypoints: list of N_persons arrays of shape (17,3) in (x, y, score) format,\n",
        "               where x,y are normalized [0,1].\n",
        "    \"\"\"\n",
        "    h, w, _ = frame.shape\n",
        "    for person in keypoints:\n",
        "        # draw keypoints\n",
        "        for idx, (x, y, c) in enumerate(person):\n",
        "            if c > conf_thresh:\n",
        "                cv2.circle(frame, (int(x * w), int(y * h)), 4, (0, 255, 0), -1)\n",
        "        # draw skeleton\n",
        "        for p1, p2 in SKELETON:\n",
        "            x1, y1, c1 = person[p1]\n",
        "            x2, y2, c2 = person[p2]\n",
        "            if c1 > conf_thresh and c2 > conf_thresh:\n",
        "                pt1 = (int(x1 * w), int(y1 * h))\n",
        "                pt2 = (int(x2 * w), int(y2 * h))\n",
        "                cv2.line(frame, pt1, pt2, (0, 255, 255), 2)\n",
        "\n",
        "frame_idx = 0\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Only run pose estimation & drawing every 5th frame\n",
        "    if frame_idx % 5 == 0:\n",
        "        # Prepare input for MoveNet\n",
        "        img = tf.image.resize_with_pad(tf.expand_dims(frame, 0), 256, 256)\n",
        "        inp = tf.cast(img, tf.int32)\n",
        "\n",
        "        # Inference\n",
        "        outputs = model.signatures['serving_default'](inp)\n",
        "        raw = outputs['output_0'].numpy()[0]  # shape: (6, 55) for up to 6 persons\n",
        "\n",
        "        # Parse the detections into a list of (17,3) arrays\n",
        "        persons = []\n",
        "        for det in raw:\n",
        "            kpts = det[:51].reshape(17, 3)\n",
        "            # MoveNet returns (y, x, score); we want (x, y, score)\n",
        "            pts = np.stack([kpts[:, 1], kpts[:, 0], kpts[:, 2]], axis=-1)\n",
        "            persons.append(pts)\n",
        "\n",
        "        # Draw on the frame\n",
        "        draw_keypoints_and_skeleton(frame, persons, conf_thresh=0.3)\n",
        "\n",
        "    # Write the (possibly annotated) frame\n",
        "    out.write(frame)\n",
        "    frame_idx += 1\n",
        "\n",
        "cap.release()\n",
        "out.release()"
      ],
      "metadata": {
        "id": "bLBJ3Pp22qYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLO POSE"
      ],
      "metadata": {
        "id": "VPneRYFP-aAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Paths\n",
        "INPUT_VIDEO  = \"/content/drive/MyDrive/smai_project/data_set/1.mp4\"\n",
        "OUTPUT_VIDEO = \"/content/drive/MyDrive/smai_project/output/1_yolo_pose.mp4\"\n",
        "YOLO_MODEL   = \"yolov8n-pose.pt\"  # or your custom .pt file\n",
        "\n",
        "# COCO-style skeleton connections\n",
        "SKELETON = [\n",
        "    (0, 1), (0, 2), (1, 3), (2, 4),\n",
        "    (0, 5), (0, 6), (5, 7), (7, 9),\n",
        "    (6, 8), (8, 10), (5, 6), (11, 12),\n",
        "    (11, 13), (13, 15), (12, 14), (14, 16)\n",
        "]\n",
        "\n",
        "# Initialize model\n",
        "model = YOLO(YOLO_MODEL)\n",
        "\n",
        "# Open input video\n",
        "cap    = cv2.VideoCapture(INPUT_VIDEO)\n",
        "fps    = cap.get(cv2.CAP_PROP_FPS)\n",
        "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Prepare output writer\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out    = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, fps, (width, height))\n",
        "\n",
        "frame_idx = 0\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    if frame_idx % 5 != 0:\n",
        "        out.write(frame)\n",
        "        frame_idx += 1\n",
        "        continue\n",
        "\n",
        "    # Inference\n",
        "    results = model(frame)[0]\n",
        "\n",
        "    # If no keypoints at all, skip\n",
        "    if results.keypoints is None or results.keypoints.data.numel() == 0:\n",
        "        out.write(frame)\n",
        "        frame_idx += 1\n",
        "        continue\n",
        "\n",
        "    # Pull out the raw (n_people,17,3) tensor\n",
        "    kp_tensor = results.keypoints.data.cpu()\n",
        "\n",
        "    # Handle single-person case: (17,3) -> (1,17,3)\n",
        "    if kp_tensor.ndim == 2 and kp_tensor.shape[1] == 3:\n",
        "        kp_tensor = kp_tensor.unsqueeze(0)\n",
        "\n",
        "    # Convert to numpy: now guaranteed (N,17,3)\n",
        "    kpts = kp_tensor.numpy()\n",
        "\n",
        "    for person in kpts:\n",
        "        for x, y, conf in person:\n",
        "            if conf > 0.3:\n",
        "                cv2.circle(frame, (int(x), int(y)), 4, (0, 255, 0), -1)\n",
        "\n",
        "        for i, j in SKELETON:\n",
        "            if i < person.shape[0] and j < person.shape[0]:\n",
        "                if person[i, 2] > 0.3 and person[j, 2] > 0.3:\n",
        "                    pt1 = (int(person[i, 0]), int(person[i, 1]))\n",
        "                    pt2 = (int(person[j, 0]), int(person[j, 1]))\n",
        "                    cv2.line(frame, pt1, pt2, (0, 255, 255), 2)\n",
        "\n",
        "    out.write(frame)\n",
        "    frame_idx += 1\n",
        "\n",
        "# Clean up\n",
        "cap.release()\n",
        "out.release()\n",
        "print(f\"Output saved to {OUTPUT_VIDEO}\")"
      ],
      "metadata": {
        "id": "TZ6G2RIx-N3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xb9px9AC-cVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing pose estimation models"
      ],
      "metadata": {
        "id": "UXFFph1j-c3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import procrustes # Import procrustes for PA\n",
        "\n",
        "# Process every Nth frame for speed\n",
        "SAMPLE_EVERY = 1\n",
        "CONF_THRESH = 0.3 # General confidence threshold, used explicitly in some areas.\n",
        "\n",
        "# Paths (edit these to your files)\n",
        "VIDEO_PATH    = \"/content/drive/MyDrive/smai_project/data_set/1.mp4\"\n",
        "GROUND_TRUTH  = \"/content/drive/MyDrive/smai_project/data_set/1.npz\"  # your uploaded .npz\n",
        "YOLO_MODEL    = \"yolov8n-pose.pt\"\n",
        "\n",
        "# COCO-style skeleton (unused for metrics in this script but good for visualization)\n",
        "SKELETON = [\n",
        "    (0, 1), (0, 2), (1, 3), (2, 4),\n",
        "    (0, 5), (0, 6), (5, 7), (7, 9),\n",
        "    (6, 8), (8, 10), (5, 6), (11, 12),\n",
        "    (11, 13), (13, 15), (12, 14), (14, 16)\n",
        "]\n",
        "\n",
        "# Mapping for common keypoints (COCO 17 keypoints) - used for consistent comparison\n",
        "# Indices for MediaPipe (33 keypoints) -> COCO (17 keypoints)\n",
        "# Note: This is an approximate mapping. You might need to adjust based on your specific keypoint definitions.\n",
        "# MediaPipe indices: https://developers.google.com/mediapipe/solutions/vision/pose_landmarker\n",
        "# COCO indices: http://cocodataset.org/#keypoints-2017\n",
        "MEDIAPIPE_TO_COCO = [\n",
        "    0,  # Nose\n",
        "    1,  # Left eye (inner) - No direct COCO equivalent, use Left eye\n",
        "    2,  # Left eye - Left eye\n",
        "    3,  # Left eye (outer) - No direct COCO equivalent, use Left eye\n",
        "    4,  # Right eye (inner) - No direct COCO equivalent, use Right eye\n",
        "    5,  # Right eye - Right eye\n",
        "    6,  # Right eye (outer) - No direct COCO equivalent, use Right eye\n",
        "    7,  # Left ear - Left ear\n",
        "    8,  # Right ear - Right ear\n",
        "    9,  # Mouth (left) - No direct COCO equivalent\n",
        "    10, # Mouth (right) - No direct COCO equivalent\n",
        "    11, # Left shoulder - Left shoulder\n",
        "    12, # Right shoulder - Right shoulder\n",
        "    13, # Left elbow - Left elbow\n",
        "    14, # Right elbow - Right elbow\n",
        "    15, # Left wrist - Left wrist\n",
        "    16, # Right wrist - Right wrist\n",
        "    17, # Left pinky 1 - No direct COCO equivalent\n",
        "    18, # Right pinky 1 - No direct COCO equivalent\n",
        "    19, # Left index 1 - No direct COCO equivalent\n",
        "    20, # Right index 1 - No direct COCO equivalent\n",
        "    21, # Left thumb 2 - No direct COCO equivalent\n",
        "    22, # Right thumb 2 - No direct COCO equivalent\n",
        "    23, # Left hip - Left hip\n",
        "    24, # Right hip - Right hip\n",
        "    25, # Left knee - Left knee\n",
        "    26, # Right knee - Right knee\n",
        "    27, # Left ankle - Left ankle\n",
        "    28, # Right ankle - Right ankle\n",
        "    29, # Left heel - No direct COCO equivalent, maybe left ankle? Using Left ankle\n",
        "    30, # Right heel - No direct COCO equivalent, maybe right ankle? Using Right ankle\n",
        "    31, # Left foot index - No direct COCO equivalent, maybe left ankle? Using Left ankle\n",
        "    32  # Right foot index - No direct COCO equivalent, maybe right ankle? Using Right ankle\n",
        "]\n",
        "\n",
        "# Create a mapping from MediaPipe index to target common index (0-16 for COCO)\n",
        "# If a MediaPipe keypoint doesn't map cleanly to a common keypoint, map it to -1\n",
        "mp_to_common_indices = [-1] * 33 # Initialize with -1 (no mapping)\n",
        "coco_keypoint_names = [\n",
        "    \"nose\", \"left_eye\", \"right_eye\", \"left_ear\", \"right_ear\",\n",
        "    \"left_shoulder\", \"right_shoulder\", \"left_elbow\", \"right_elbow\",\n",
        "    \"left_wrist\", \"right_wrist\", \"left_hip\", \"right_hip\",\n",
        "    \"left_knee\", \"right_knee\", \"left_ankle\", \"right_ankle\"\n",
        "]\n",
        "\n",
        "# Populate the mapping based on common sense and typical uses\n",
        "mp_to_common_indices[0] = 0  # Nose\n",
        "mp_to_common_indices[2] = 1  # Left eye\n",
        "mp_to_common_indices[5] = 2  # Right eye\n",
        "mp_to_common_indices[7] = 3  # Left ear\n",
        "mp_to_common_indices[8] = 4  # Right ear\n",
        "mp_to_common_indices[11] = 5 # Left shoulder\n",
        "mp_to_common_indices[12] = 6 # Right shoulder\n",
        "mp_to_common_indices[13] = 7 # Left elbow\n",
        "mp_to_common_indices[14] = 8 # Right elbow\n",
        "mp_to_common_indices[15] = 9 # Left wrist\n",
        "mp_to_common_indices[16] = 10 # Right wrist\n",
        "mp_to_common_indices[23] = 11 # Left hip\n",
        "mp_to_common_indices[24] = 12 # Right hip\n",
        "mp_to_common_indices[25] = 13 # Left knee\n",
        "mp_to_common_indices[26] = 14 # Right knee\n",
        "mp_to_common_indices[27] = 15 # Left ankle\n",
        "mp_to_common_indices[28] = 16 # Right ankle\n",
        "\n",
        "# MoveNet and YOLO use the COCO 17 keypoints directly.\n",
        "# YOLO keypoint order: https://docs.ultralytics.com/tasks/pose/\n",
        "# MoveNet keypoint order: https://www.tensorflow.org/hub/tutorials/movenet\n",
        "# They match the COCO order.\n",
        "\n",
        "def extract_mediapipe(video_path):\n",
        "    \"\"\"\n",
        "    Extracts pose keypoints from a video using MediaPipe Pose.\n",
        "    Returns an array of shape (num_sampled_frames, 33, 3) for (x, y, visibility).\n",
        "    Coordinates are normalized (0.0 to 1.0).\n",
        "    \"\"\"\n",
        "    mp_pose_solution = mp.solutions.pose\n",
        "    # Initialize with higher confidence for potentially better quality, adjust as needed\n",
        "    # Setting static_image_mode=False for video processing\n",
        "    pose = mp_pose_solution.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video {video_path}\")\n",
        "        return np.array([])\n",
        "\n",
        "    frame_idx = 0\n",
        "    keypoints_list = []\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if frame_idx % SAMPLE_EVERY == 0:\n",
        "            # Convert BGR frame to RGB for MediaPipe\n",
        "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            # Process the frame. Pass by reference for performance.\n",
        "            results = pose.process(rgb_frame)\n",
        "\n",
        "            if results.pose_landmarks:\n",
        "                # Extract x, y, and visibility for each landmark\n",
        "                # MediaPipe gives normalized coordinates directly\n",
        "                pts = [(lm.x, lm.y, lm.visibility) for lm in results.pose_landmarks.landmark]\n",
        "                keypoints_list.append(np.array(pts))\n",
        "            else:\n",
        "                # Append NaNs for 33 landmarks if no pose detected\n",
        "                keypoints_list.append(np.full((33, 3), np.nan))\n",
        "        frame_idx += 1\n",
        "    cap.release()\n",
        "    pose.close() # Release MediaPipe resources\n",
        "    if not keypoints_list:\n",
        "        return np.array([])\n",
        "    return np.stack(keypoints_list)\n",
        "\n",
        "\n",
        "def extract_movenet(video_path):\n",
        "    \"\"\"\n",
        "    Extracts pose keypoints from a video using MoveNet SinglePose Lightning.\n",
        "    Returns an array of shape (num_sampled_frames, 17, 3) for (x, y, score).\n",
        "    Coordinates are normalized (0.0 to 1.0).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the MoveNet model from TensorFlow Hub\n",
        "        # Consider caching the model if running multiple times\n",
        "        model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "        input_signature = model.signatures['serving_default']\n",
        "        input_size = 192 # Lightning model input size\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading MoveNet model: {e}\")\n",
        "        return np.array([])\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video {video_path}\")\n",
        "        return np.array([])\n",
        "\n",
        "    frame_idx, keypoints_list = 0, []\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if frame_idx % SAMPLE_EVERY == 0:\n",
        "            # 1. Convert BGR frame to RGB\n",
        "            rgb_cv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            # 2. Convert to TensorFlow tensor, ensure dtype is uint8 for resize_with_pad\n",
        "            img_tensor_uint8 = tf.convert_to_tensor(rgb_cv_frame, dtype=tf.uint8)\n",
        "\n",
        "            # 3. Resize and pad the image to keep aspect ratio and fit input size.\n",
        "            # MoveNet expects input shape (1, height, width, 3) and type int32\n",
        "            input_image_resized = tf.image.resize_with_pad(img_tensor_uint8, input_size, input_size)\n",
        "\n",
        "            # 4. Cast to int32 for the model and add batch dimension.\n",
        "            input_batch = tf.expand_dims(tf.cast(input_image_resized, dtype=tf.int32), axis=0)\n",
        "\n",
        "            try:\n",
        "                # Run inference\n",
        "                outputs = input_signature(input=input_batch)\n",
        "                # output_0 shape: (1, 1, 17, 3) -> (batch, person, keypoint, (y, x, score))\n",
        "                kpts = outputs['output_0'].numpy()[0, 0, :, :] # (17, 3) -> (y, x, score)\n",
        "                # Convert from (y, x, score) to (x, y, score)\n",
        "                # Normalize coordinates to original image size (if needed, but usually comparison is done normalized 0-1)\n",
        "                # The coordinates from Movenet are normalized to the padded input size (192x192), not the original frame.\n",
        "                # For comparison with ground truth normalized to original frame, normalization relative to original frame is needed.\n",
        "                # A simpler approach for comparison is to keep them normalized 0-1 relative to the model's effective input area.\n",
        "                # However, if ground truth is normalized to original frame, the predicted keypoints should also be normalized to the original frame dimensions.\n",
        "                # Let's re-normalize to the original frame dimensions (width, height)\n",
        "                frame_height, frame_width, _ = frame.shape\n",
        "                kpts_renormalized = kpts.copy()\n",
        "                kpts_renormalized[:, 1] *= (frame_width / input_size) # Renormalize x (originally y)\n",
        "                kpts_renormalized[:, 0] *= (frame_height / input_size) # Renormalize y (originally x)\n",
        "\n",
        "                pts = np.stack([kpts_renormalized[:, 1], kpts_renormalized[:, 0], kpts_renormalized[:, 2]], axis=-1) # (x, y, score)\n",
        "\n",
        "                keypoints_list.append(pts)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during MoveNet inference on frame {frame_idx}: {e}\")\n",
        "                # Append NaNs for 17 landmarks if inference fails\n",
        "                keypoints_list.append(np.full((17, 3), np.nan))\n",
        "        frame_idx += 1\n",
        "    cap.release()\n",
        "    # Note: TensorFlow Hub models don't have a explicit .close() method like MediaPipe\n",
        "    if not keypoints_list:\n",
        "        return np.array([])\n",
        "    return np.stack(keypoints_list)\n",
        "\n",
        "\n",
        "def extract_yolo(video_path, model_path):\n",
        "    \"\"\"\n",
        "    Extracts pose keypoints from a video using YOLOv8-Pose.\n",
        "    Returns an array of shape (num_sampled_frames, 17, 3) for (x, y, confidence).\n",
        "    Coordinates are initially in PIXEL values. They will be normalized in run_all().\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = YOLO(model_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading YOLO model from {model_path}: {e}\")\n",
        "        return np.array([])\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video {video_path}\")\n",
        "        return np.array([])\n",
        "\n",
        "    frame_idx, keypoints_list = 0, []\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if frame_idx % SAMPLE_EVERY == 0:\n",
        "            # Running inference with conf threshold\n",
        "            results = model(frame, verbose=False, conf=CONF_THRESH)\n",
        "\n",
        "            if results and results[0].keypoints is not None and results[0].keypoints.data.numel() > 0:\n",
        "                # Keypoints data shape: [NumPersons, NumKeypoints, (x,y,conf)]\n",
        "                kp_tensor = results[0].keypoints.data.cpu().numpy()\n",
        "\n",
        "                if kp_tensor.ndim == 3 and kp_tensor.shape[0] > 0:\n",
        "                    if kp_tensor.shape[0] == 1: # Only one person detected\n",
        "                        person_keypoints = kp_tensor[0] # Shape [17, 3]\n",
        "                    else: # Multiple persons, select the one with highest average keypoint confidence\n",
        "                        # Consider only keypoints with confidence > CONF_THRESH for this average\n",
        "                        # Handle cases where all keypoints for a person are below threshold\n",
        "                        confidences = kp_tensor[:, :, 2]\n",
        "                        # Replace confidences below threshold with NaN for averaging\n",
        "                        confidences_thresholded = np.where(confidences > CONF_THRESH, confidences, np.nan)\n",
        "                        avg_confidences = np.nanmean(confidences_thresholded, axis=1) # Avg conf per person, ignoring NaNs\n",
        "\n",
        "                        if np.all(np.isnan(avg_confidences)): # If no person has any keypoint above threshold\n",
        "                             person_keypoints = np.full((17, 3), np.nan)\n",
        "                        else:\n",
        "                            best_person_idx = np.nanargmax(avg_confidences)\n",
        "                            person_keypoints = kp_tensor[best_person_idx] # Shape [17, 3]\n",
        "\n",
        "                    # Set keypoints with confidence below threshold to NaN for metric calculation\n",
        "                    # This ensures only confident predictions contribute to error\n",
        "                    person_keypoints[person_keypoints[:, 2] < CONF_THRESH, :2] = np.nan # Set (x,y) to NaN if confidence < threshold\n",
        "\n",
        "                    keypoints_list.append(person_keypoints)\n",
        "                else: # Fallback for unexpected shapes\n",
        "                    keypoints_list.append(np.full((17, 3), np.nan))\n",
        "            else:\n",
        "                # If no pose is detected, append NaNs (YOLO default 17 keypoints)\n",
        "                keypoints_list.append(np.full((17, 3), np.nan))\n",
        "        frame_idx += 1\n",
        "    cap.release()\n",
        "    # YOLO model doesn't have an explicit close method like MediaPipe\n",
        "    if not keypoints_list:\n",
        "        return np.array([])\n",
        "    return np.stack(keypoints_list)\n",
        "\n",
        "\n",
        "def map_mediapipe_to_common(mp_kpts, common_indices_map, num_common_kpts):\n",
        "    \"\"\"\n",
        "    Maps MediaPipe keypoints (33) to a common set of keypoints (e.g., COCO 17).\n",
        "    Returns an array of shape (num_frames, num_common_kpts, 3).\n",
        "    \"\"\"\n",
        "    if mp_kpts.size == 0:\n",
        "        return np.full((0, num_common_kpts, 3), np.nan)\n",
        "\n",
        "    num_frames = mp_kpts.shape[0]\n",
        "    mapped_kpts = np.full((num_frames, num_common_kpts, 3), np.nan)\n",
        "\n",
        "    for common_idx in range(num_common_kpts):\n",
        "        # Find the corresponding MediaPipe index\n",
        "        try:\n",
        "            mp_idx = common_indices_map.index(common_idx)\n",
        "            mapped_kpts[:, common_idx, :] = mp_kpts[:, mp_idx, :]\n",
        "        except ValueError:\n",
        "            # If no MediaPipe index maps to this common index, it remains NaN\n",
        "            pass\n",
        "    return mapped_kpts\n",
        "\n",
        "\n",
        "def calculate_mpjpe_2d(gt_kpts, pred_kpts):\n",
        "    \"\"\"\n",
        "    Calculates the 2D Mean Per Joint Position Error (MPJPE) between ground truth and predicted keypoints.\n",
        "    Assumes inputs are [num_frames, num_keypoints, 2 (x,y)].\n",
        "    Returns a single scalar value (mean error across all valid joints and frames).\n",
        "    \"\"\"\n",
        "    if gt_kpts.size == 0 or pred_kpts.size == 0:\n",
        "        print(\"Warning: Ground truth or predicted keypoints are empty for MPJPE calculation.\")\n",
        "        return np.nan\n",
        "\n",
        "    # Ensure inputs have only x, y coordinates\n",
        "    if gt_kpts.shape[-1] > 2:\n",
        "        gt_kpts = gt_kpts[..., :2]\n",
        "    if pred_kpts.shape[-1] > 2:\n",
        "        pred_kpts = pred_kpts[..., :2]\n",
        "\n",
        "    # Ensure shapes match for comparison (frames, keypoints, 2)\n",
        "    min_frames = min(gt_kpts.shape[0], pred_kpts.shape[0])\n",
        "    min_keypoints = min(gt_kpts.shape[1], pred_kpts.shape[1])\n",
        "\n",
        "    if min_frames == 0 or min_keypoints == 0:\n",
        "        print(\"Warning: No common frames or keypoints for MPJPE calculation.\")\n",
        "        return np.nan\n",
        "\n",
        "    gt_subset = gt_kpts[:min_frames, :min_keypoints, :]\n",
        "    pred_subset = pred_kpts[:min_frames, :min_keypoints, :]\n",
        "\n",
        "    # Calculate Euclidean distance for each keypoint in each frame\n",
        "    dists = np.linalg.norm(pred_subset - gt_subset, axis=-1) # Shape [num_frames, num_keypoints]\n",
        "\n",
        "    # Calculate mean over all valid distances (ignoring NaNs if any)\n",
        "    mean_mpjpe = np.nanmean(dists)\n",
        "\n",
        "    return mean_mpjpe\n",
        "\n",
        "\n",
        "def calculate_pa_mpjpe_2d(gt_kpts, pred_kpts):\n",
        "    \"\"\"\n",
        "    Calculates the 2D Procrustes Analysis Mean Per Joint Position Error (PA-MPJPE).\n",
        "    Aligns predicted pose to ground truth pose for each frame using 2D Procrustes analysis\n",
        "    and then calculates MPJPE.\n",
        "    Assumes inputs are [num_frames, num_keypoints, 2 (x,y)].\n",
        "    Returns a single scalar value (mean error across all valid joints and frames after alignment).\n",
        "    \"\"\"\n",
        "    if gt_kpts.size == 0 or pred_kpts.size == 0:\n",
        "        print(\"Warning: Ground truth or predicted keypoints are empty for PA-MPJPE calculation.\")\n",
        "        return np.nan\n",
        "\n",
        "    # Ensure inputs have only x, y coordinates\n",
        "    if gt_kpts.shape[-1] > 2:\n",
        "        gt_kpts = gt_kpts[..., :2]\n",
        "    if pred_kpts.shape[-1] > 2:\n",
        "        pred_kpts = pred_kpts[..., :2]\n",
        "\n",
        "    min_frames = min(gt_kpts.shape[0], pred_kpts.shape[0])\n",
        "    min_keypoints = min(gt_kpts.shape[1], pred_kpts.shape[1])\n",
        "\n",
        "    if min_frames == 0 or min_keypoints == 0:\n",
        "        print(\"Warning: No common frames or keypoints for PA-MPJPE calculation.\")\n",
        "        return np.nan\n",
        "\n",
        "    gt_subset = gt_kpts[:min_frames, :min_keypoints, :]\n",
        "    pred_subset = pred_kpts[:min_frames, :min_keypoints, :]\n",
        "\n",
        "    aligned_dists_list = []\n",
        "\n",
        "    for i in range(min_frames):\n",
        "        gt_frame = gt_subset[i] # Shape [num_keypoints, 2]\n",
        "        pred_frame = pred_subset[i] # Shape [num_keypoints, 2]\n",
        "\n",
        "        # Handle frames with insufficient valid keypoints for Procrustes\n",
        "        valid_indices = np.all(~np.isnan([gt_frame, pred_frame]), axis=(0, 2))\n",
        "\n",
        "        if np.sum(valid_indices) < 2: # Need at least 2 valid points for Procrustes\n",
        "            # Cannot perform Procrustes analysis for this frame\n",
        "            aligned_dists_list.append(np.full(min_keypoints, np.nan))\n",
        "            continue\n",
        "\n",
        "        gt_valid = gt_frame[valid_indices]\n",
        "        pred_valid = pred_frame[valid_indices]\n",
        "\n",
        "        # Perform 2D Procrustes analysis\n",
        "        # procrustes(data1, data2) returns (mtx1, mtx2, disparity)\n",
        "        # mtx1 is the aligned version of data1\n",
        "        # mtx2 is the aligned version of data2\n",
        "        # disparity is the Procrustes distance between the two sets (after optimal transformation)\n",
        "        # We are interested in the aligned predicted pose (mtx2) and the original ground truth (mtx1)\n",
        "        # The disparity is effectively the PA-MPJPE for this frame across the valid points.\n",
        "        # To get per-joint error for averaging, we can use the aligned points directly.\n",
        "        try:\n",
        "             # Use scipy.spatial.procrustes which handles 2D data correctly\n",
        "             # Note: scipy procrustes aligns data2 to data1. So pred_valid is aligned to gt_valid.\n",
        "             mtx_gt_aligned, mtx_pred_aligned, disparity = procrustes(gt_valid, pred_valid)\n",
        "\n",
        "             # Calculate per-joint distances after alignment for the valid points\n",
        "             aligned_dists_valid = np.linalg.norm(mtx_pred_aligned - mtx_gt_aligned, axis=-1) # Shape [num_valid_keypoints]\n",
        "\n",
        "             # Map the aligned distances back to the original keypoint indices\n",
        "             aligned_dists_frame = np.full(min_keypoints, np.nan)\n",
        "             aligned_dists_frame[valid_indices] = aligned_dists_valid\n",
        "             aligned_dists_list.append(aligned_dists_frame)\n",
        "\n",
        "        except ValueError as e:\n",
        "             print(f\"Warning: Procrustes analysis failed for frame {i}. Error: {e}. Skipping frame.\")\n",
        "             aligned_dists_list.append(np.full(min_keypoints, np.nan))\n",
        "        except Exception as e:\n",
        "             print(f\"An unexpected error occurred during Procrustes analysis for frame {i}: {e}. Skipping frame.\")\n",
        "             aligned_dists_list.append(np.full(min_keypoints, np.nan))\n",
        "\n",
        "\n",
        "    if not aligned_dists_list:\n",
        "        return np.nan # Should not happen if min_frames > 0, but as a safeguard\n",
        "\n",
        "    aligned_dists_all_frames = np.stack(aligned_dists_list) # Shape [num_frames, num_keypoints]\n",
        "\n",
        "    # Calculate mean over all valid aligned distances\n",
        "    mean_pa_mpjpe = np.nanmean(aligned_dists_all_frames)\n",
        "\n",
        "    return mean_pa_mpjpe\n",
        "\n",
        "\n",
        "def run_all():\n",
        "    # Optional: TensorFlow GPU diagnosis\n",
        "    # print(\"TensorFlow version:\", tf.__version__)\n",
        "    # physical_devices = tf.config.list_physical_devices('GPU')\n",
        "    # print(\"Num GPUs Available: \", len(physical_devices))\n",
        "    # if physical_devices:\n",
        "    #     try:\n",
        "    #         for gpu in physical_devices:\n",
        "    #             tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    #         print(\"GPU memory growth enabled.\")\n",
        "    #     except RuntimeError as e:\n",
        "    #         print(f\"Could not set memory growth (this is okay if already set or on CPU): {e}\")\n",
        "    # else:\n",
        "    #     print(\"No GPU detected by TensorFlow. Running on CPU.\")\n",
        "    # # To force CPU for MoveNet if DNN errors persist (for testing):\n",
        "    # # tf.config.set_visible_devices([], 'GPU')\n",
        "    # # print(\"Attempting to force CPU for TensorFlow operations.\")\n",
        "\n",
        "\n",
        "    # Number of common keypoints for comparison (COCO 17)\n",
        "    NUM_COMMON_KEYPOINTS = 17\n",
        "\n",
        "    # Load ground truth\n",
        "    try:\n",
        "        data = np.load(GROUND_TRUTH)\n",
        "        # Assuming the first array in the npz is the keypoints\n",
        "        gt_keypoints_all_frames = data[list(data.keys())[0]]\n",
        "        # Sample ground truth keypoints\n",
        "        gt_keypoints = gt_keypoints_all_frames[::SAMPLE_EVERY, :, :]\n",
        "        print(f\"Ground truth keypoints loaded and sampled. Shape: {gt_keypoints.shape}\")\n",
        "\n",
        "        # Ensure ground truth has at least 2 components (x, y)\n",
        "        if gt_keypoints.ndim < 3 or gt_keypoints.shape[2] < 2:\n",
        "            print(f\"Error: Ground truth keypoints do not have expected shape (frames, keypoints, >= 2 components). Actual shape: {gt_keypoints.shape}\")\n",
        "            return\n",
        "\n",
        "        # Use the number of keypoints in the ground truth for determining common keypoints if it's less than 17\n",
        "        # This prevents errors if the ground truth has fewer keypoints than the models.\n",
        "        actual_gt_keypoints = gt_keypoints.shape[1]\n",
        "        if actual_gt_keypoints < NUM_COMMON_KEYPOINTS:\n",
        "             print(f\"Warning: Ground truth has {actual_gt_keypoints} keypoints, less than the target {NUM_COMMON_KEYPOINTS}. Metrics will be based on {actual_gt_keypoints} keypoints.\")\n",
        "             NUM_COMMON_KEYPOINTS_FOR_METRICS = actual_gt_keypoints\n",
        "        else:\n",
        "             NUM_COMMON_KEYPOINTS_FOR_METRICS = NUM_COMMON_KEYPOINTS\n",
        "\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Ground truth file not found at {GROUND_TRUTH}\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or sampling ground truth from {GROUND_TRUTH}: {e}\")\n",
        "        return\n",
        "\n",
        "    print(\"Extracting MediaPipe keypoints...\")\n",
        "    kp_mp_full = extract_mediapipe(VIDEO_PATH)\n",
        "    # Map MediaPipe keypoints to the common set (e.g., COCO 17)\n",
        "    kp_mp = map_mediapipe_to_common(kp_mp_full, mp_to_common_indices, NUM_COMMON_KEYPOINTS)\n",
        "\n",
        "    if kp_mp.size > 0:\n",
        "        np.savez(\"mediapipe_keypoints_mapped.npz\", kp_mp)\n",
        "        print(f\"MediaPipe keypoints extracted and mapped. Shape: {kp_mp.shape if kp_mp.size > 0 else 'Empty'}\")\n",
        "    else:\n",
        "        print(\"MediaPipe keypoint extraction resulted in an empty array.\")\n",
        "\n",
        "\n",
        "    print(\"Extracting MoveNet keypoints...\")\n",
        "    kp_mn = extract_movenet(VIDEO_PATH)\n",
        "    if kp_mn.size > 0:\n",
        "        # Ensure MoveNet also has confidence/score for filtering\n",
        "        if kp_mn.shape[-1] < 3:\n",
        "             print(\"Warning: MoveNet keypoints do not have confidence scores. Assuming confidence 1.0 for all.\")\n",
        "             kp_mn_temp = np.ones((kp_mn.shape[0], kp_mn.shape[1], 3))\n",
        "             kp_mn_temp[:,:,:2] = kp_mn[:,:,:2]\n",
        "             kp_mn = kp_mn_temp\n",
        "\n",
        "        # Set keypoints with confidence below threshold to NaN for metric calculation\n",
        "        kp_mn[kp_mn[:, :, 2] < CONF_THRESH, :2] = np.nan\n",
        "\n",
        "        np.savez(\"movenet_keypoints.npz\", kp_mn)\n",
        "        print(f\"MoveNet keypoints extracted. Shape: {kp_mn.shape if kp_mn.size > 0 else 'Empty'}\")\n",
        "    else:\n",
        "        print(\"MoveNet keypoint extraction resulted in an empty array.\")\n",
        "\n",
        "\n",
        "    print(\"Extracting YOLO-Pose keypoints (pixel coordinates initially)...\")\n",
        "    kp_yo_pixels = extract_yolo(VIDEO_PATH, YOLO_MODEL)\n",
        "    kp_yo_normalized = np.array([]) # Initialize as empty array\n",
        "\n",
        "    if kp_yo_pixels.size > 0:\n",
        "         # Normalize YOLO keypoints if extraction was successful\n",
        "         cap_temp = cv2.VideoCapture(VIDEO_PATH)\n",
        "         if cap_temp.isOpened():\n",
        "             width = int(cap_temp.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "             height = int(cap_temp.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "             cap_temp.release()\n",
        "\n",
        "             if width > 0 and height > 0:\n",
        "                 kp_yo_normalized = kp_yo_pixels.copy()\n",
        "                 # Normalize x and y coordinates to be between 0 and 1\n",
        "                 kp_yo_normalized[..., 0] /= width  # Normalize x\n",
        "                 kp_yo_normalized[..., 1] /= height # Normalize y\n",
        "                 # Confidence (kp_yo_normalized[..., 2]) remains unchanged\n",
        "                 np.savez(\"yolo_keypoints_normalized.npz\", kp_yo_normalized)\n",
        "                 print(f\"YOLO-Pose keypoints extracted and normalized. Shape: {kp_yo_normalized.shape}\")\n",
        "             else:\n",
        "                 print(\"Error: Could not get valid video dimensions for YOLO normalization.\")\n",
        "                 # If dimensions are invalid, normalization is not possible, set normalized kpts to NaN\n",
        "                 kp_yo_normalized = np.full_like(kp_yo_pixels, np.nan)\n",
        "         else:\n",
        "             print(\"Error: Could not open video to get dimensions for YOLO normalization.\")\n",
        "             # If video cannot be opened, set normalized kpts to NaN\n",
        "             kp_yo_normalized = np.full_like(kp_yo_pixels, np.nan)\n",
        "    else:\n",
        "        print(\"YOLO-Pose keypoint extraction resulted in an empty array.\")\n",
        "\n",
        "\n",
        "    # Prepare ground truth for comparison (slice to common keypoints and only take x, y)\n",
        "    gt_xy_common = gt_keypoints[:, :NUM_COMMON_KEYPOINTS_FOR_METRICS, :2]\n",
        "\n",
        "    # Prepare predicted keypoints for comparison (slice to common keypoints and only take x, y)\n",
        "    # Handle potential differences in number of keypoints returned by models if not already 17\n",
        "    kp_mp_xy_common = kp_mp[:, :NUM_COMMON_KEYPOINTS_FOR_METRICS, :2] if kp_mp.size > 0 else np.full((gt_xy_common.shape[0], NUM_COMMON_KEYPOINTS_FOR_METRICS, 2), np.nan)\n",
        "    kp_mn_xy_common = kp_mn[:, :NUM_COMMON_KEYPOINTS_FOR_METRICS, :2] if kp_mn.size > 0 else np.full((gt_xy_common.shape[0], NUM_COMMON_KEYPOINTS_FOR_METRICS, 2), np.nan)\n",
        "    kp_yo_xy_common = kp_yo_normalized[:, :NUM_COMMON_KEYPOINTS_FOR_METRICS, :2] if kp_yo_normalized.size > 0 else np.full((gt_xy_common.shape[0], NUM_COMMON_KEYPOINTS_FOR_METRICS, 2), np.nan)\n",
        "\n",
        "\n",
        "    # Compute MPJPE (2D adaptation)\n",
        "    print(\"\\nComputing 2D MPJPE...\")\n",
        "    mpjpe_mp = calculate_mpjpe_2d(gt_xy_common, kp_mp_xy_common)\n",
        "    mpjpe_mn = calculate_mpjpe_2d(gt_xy_common, kp_mn_xy_common)\n",
        "    mpjpe_yo = calculate_mpjpe_2d(gt_xy_common, kp_yo_xy_common)\n",
        "\n",
        "    print(f\"2D MPJPE Results:\")\n",
        "    print(f\"  MediaPipe: {mpjpe_mp:.4f}\")\n",
        "    print(f\"  MoveNet:   {mpjpe_mn:.4f}\")\n",
        "    print(f\"  YOLO-Pose: {mpjpe_yo:.4f}\")\n",
        "\n",
        "    # Compute PA-MPJPE (2D adaptation)\n",
        "    print(\"\\nComputing 2D PA-MPJPE...\")\n",
        "    pa_mpjpe_mp = calculate_pa_mpjpe_2d(gt_xy_common, kp_mp_xy_common)\n",
        "    pa_mpjpe_mn = calculate_pa_mpjpe_2d(gt_xy_common, kp_mn_xy_common)\n",
        "    pa_mpjpe_yo = calculate_pa_mpjpe_2d(gt_xy_common, kp_yo_xy_common)\n",
        "\n",
        "    print(f\"2D PA-MPJPE Results:\")\n",
        "    print(f\"  MediaPipe: {pa_mpjpe_mp:.4f}\")\n",
        "    print(f\"  MoveNet:   {pa_mpjpe_mn:.4f}\")\n",
        "    print(f\"  YOLO-Pose: {pa_mpjpe_yo:.4f}\")\n",
        "\n",
        "    # --- Original Similarity and Distance Metrics (using compute_metrics) ---\n",
        "    # The original compute_metrics calculated mean distance and a simple similarity score\n",
        "    # per joint. We can keep this for a per-joint view.\n",
        "    # We need to ensure the inputs to compute_metrics also have the confidence/score dimension\n",
        "    # if needed, but the current implementation only uses x, y for distance/similarity.\n",
        "    # So, passing the sliced and potentially NaN'd keypoints (shape [frames, common_kpts, 3]) is fine.\n",
        "    print(\"\\nComputing per-joint distance and similarity (original metrics)...\")\n",
        "    dist_mp, sim_mp = compute_metrics(gt_keypoints[:, :NUM_COMMON_KEYPOINTS_FOR_METRICS, :], kp_mp[:, :NUM_COMMON_KEYPOINTS_FOR_METRICS, :], NUM_COMMON_KEYPOINTS_FOR_METRICS)\n",
        "    dist_mn, sim_mn = compute_metrics(gt_keypoints[:, :NUM_COMMON_KEYPOINTS_FOR_METRICS, :], kp_mn[:, :NUM_COMMON_KEYPOINTS_FOR_METRICS, :], NUM_COMMON_KEYPOINTS_FOR_METRICS)\n",
        "    dist_yo, sim_yo = compute_metrics(gt_keypoints[:, :NUM_COMMON_KEYPOINTS_FOR_METRICS, :], kp_yo_normalized[:, :NUM_COMMON_KEYPOINTS_FOR_METRICS, :], NUM_COMMON_KEYPOINTS_FOR_METRICS)\n",
        "\n",
        "\n",
        "    # Calculate overall average of the original metrics\n",
        "    avg_dist = [np.nanmean(dist_mp), np.nanmean(dist_mn), np.nanmean(dist_yo)]\n",
        "    avg_sim  = [np.nanmean(sim_mp),  np.nanmean(sim_mn),  np.nanmean(sim_yo)]\n",
        "    methods  = ['MediaPipe', 'MoveNet', 'YOLO-Pose']\n",
        "\n",
        "    print(f\"\\nOverall Average Original Metrics:\")\n",
        "    print(f\"  Average Distances: MediaPipe={avg_dist[0]:.4f}, MoveNet={avg_dist[1]:.4f}, YOLO={avg_dist[2]:.4f}\")\n",
        "    print(f\"  Average Similarities: MediaPipe={avg_sim[0]:.4f}, MoveNet={avg_sim[1]:.4f}, YOLO={avg_sim[2]:.4f}\\n\")\n",
        "\n",
        "\n",
        "    # Plotting - Updated to potentially use the actual number of common keypoints\n",
        "    joints_for_plot = np.arange(NUM_COMMON_KEYPOINTS_FOR_METRICS)\n",
        "    joint_labels = [str(j) for j in joints_for_plot] # Simple index labels for now\n",
        "\n",
        "    if not (np.all(np.isnan(sim_mp)) and np.all(np.isnan(sim_mn)) and np.all(np.isnan(sim_yo))):\n",
        "        plt.figure(figsize=(14, 7))\n",
        "        w = 0.25\n",
        "        plt.bar(joints_for_plot - w, sim_mp, w, label='MediaPipe', alpha=0.8)\n",
        "        plt.bar(joints_for_plot,     sim_mn, w, label='MoveNet', alpha=0.8)\n",
        "        plt.bar(joints_for_plot + w, sim_yo, w, label='YOLO-Pose', alpha=0.8)\n",
        "        plt.xlabel('Joint Index', fontsize=12); plt.ylabel('Similarity Score', fontsize=12)\n",
        "        plt.title('Pose Estimation Similarity per Joint', fontsize=14)\n",
        "        plt.xticks(joints_for_plot, joint_labels, fontsize=10); plt.yticks(fontsize=10)\n",
        "        plt.legend(fontsize=10); plt.tight_layout();\n",
        "        plt.savefig('similarity_per_joint.png')\n",
        "        print(\"Saved similarity_per_joint.png\")\n",
        "        plt.close()\n",
        "\n",
        "    if not (np.all(np.isnan(dist_mp)) and np.all(np.isnan(dist_mn)) and np.all(np.isnan(dist_yo))):\n",
        "        plt.figure(figsize=(14, 7))\n",
        "        w = 0.25\n",
        "        plt.bar(joints_for_plot - w, dist_mp, w, label='MediaPipe', alpha=0.8)\n",
        "        plt.bar(joints_for_plot,     dist_mn, w, label='MoveNet', alpha=0.8)\n",
        "        plt.bar(joints_for_plot + w, dist_yo, w, label='YOLO-Pose', alpha=0.8)\n",
        "        plt.xlabel('Joint Index', fontsize=12); plt.ylabel('Average Euclidean Distance', fontsize=12)\n",
        "        plt.title('Pose Estimation Average Distance per Joint', fontsize=14)\n",
        "        plt.xticks(joints_for_plot, joint_labels, fontsize=10); plt.yticks(fontsize=10)\n",
        "        plt.legend(fontsize=10); plt.tight_layout()\n",
        "        plt.savefig('distance_per_joint.png')\n",
        "        print(\"Saved distance_per_joint.png\")\n",
        "        plt.close()\n",
        "\n",
        "    if not (np.all(np.isnan(avg_dist)) and np.all(np.isnan(avg_sim))):\n",
        "        idx = np.arange(len(methods))\n",
        "        fig, ax1 = plt.subplots(figsize=(10, 7))\n",
        "        bar_width_avg = 0.35\n",
        "\n",
        "        color_dist = 'skyblue'\n",
        "        ax1.set_xlabel('Method', fontsize=12)\n",
        "        ax1.set_ylabel('Average Distance (Original Metric)', color=color_dist, fontsize=12)\n",
        "        bars1 = ax1.bar(idx - bar_width_avg/2, avg_dist, bar_width_avg, label='Avg Distance', alpha=0.8, color=color_dist)\n",
        "        ax1.tick_params(axis='y', labelcolor=color_dist)\n",
        "        ax1.set_xticks(idx)\n",
        "        ax1.set_xticklabels(methods, fontsize=10)\n",
        "\n",
        "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "        color_sim = 'salmon'\n",
        "        ax2.set_ylabel('Average Similarity (Original Metric)', color=color_sim, fontsize=12)\n",
        "        bars2 = ax2.bar(idx + bar_width_avg/2, avg_sim,  bar_width_avg, label='Avg Similarity', alpha=0.8, color=color_sim)\n",
        "        ax2.tick_params(axis='y', labelcolor=color_sim)\n",
        "\n",
        "        # Add MPJPE and PA-MPJPE to a table or text output instead of a separate plot\n",
        "        # because they are overall metrics, not per-joint.\n",
        "        mpjpe_values = [mpjpe_mp, mpjpe_mn, mpjpe_yo]\n",
        "        pa_mpjpe_values = [pa_mpjpe_mp, pa_mpjpe_mn, pa_mpjpe_yo]\n",
        "\n",
        "        # Combine legend handles and labels from both axes\n",
        "        lines, labels = ax1.get_legend_handles_labels()\n",
        "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "        ax2.legend(lines + lines2, labels + labels2, loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=2, fontsize=10) # Adjust bbox_to_anchor\n",
        "\n",
        "        plt.title('Overall Average Original Metrics per Method', fontsize=14)\n",
        "        fig.tight_layout(rect=[0, 0.1, 1, 1]) # Adjust layout to make space for legend below\n",
        "\n",
        "        plt.savefig('average_original_metrics.png', bbox_inches='tight')\n",
        "        print(\"Saved average_original_metrics.png\")\n",
        "        plt.close()\n",
        "\n",
        "    print(\"\\nDone! Plots saved to disk (if metrics were valid).\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_all()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4yT2Qft2qvq",
        "outputId": "37e5b4cb-f5a9-4f0e-fb7f-70dcefed3591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground truth keypoints loaded and sampled. Shape: (4450, 17, 3)\n",
            "Extracting MediaPipe keypoints...\n",
            "MediaPipe keypoints extracted and mapped. Shape: (4450, 17, 3)\n",
            "Extracting MoveNet keypoints...\n",
            "MoveNet keypoints extracted. Shape: (4450, 17, 3)\n",
            "Extracting YOLO-Pose keypoints (pixel coordinates initially)...\n",
            "YOLO-Pose keypoints extracted and normalized. Shape: (4450, 17, 3)\n",
            "\n",
            "Computing 2D MPJPE...\n",
            "2D MPJPE Results:\n",
            "  MediaPipe: 0.7717\n",
            "  MoveNet:   5.6846\n",
            "  YOLO-Pose: 0.7225\n",
            "\n",
            "Computing 2D PA-MPJPE...\n",
            "2D PA-MPJPE Results:\n",
            "  MediaPipe: 0.1909\n",
            "  MoveNet:   0.2099\n",
            "  YOLO-Pose: 0.2010\n",
            "\n",
            "Computing per-joint distance and similarity (original metrics)...\n",
            "\n",
            "Overall Average Original Metrics:\n",
            "  Average Distances: MediaPipe=0.7717, MoveNet=5.6888, YOLO=0.7107\n",
            "  Average Similarities: MediaPipe=0.5699, MoveNet=0.1497, YOLO=0.5906\n",
            "\n",
            "Saved similarity_per_joint.png\n",
            "Saved distance_per_joint.png\n",
            "Saved average_original_metrics.png\n",
            "\n",
            "Done! Plots saved to disk (if metrics were valid).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bb8hs7kS2uq7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}