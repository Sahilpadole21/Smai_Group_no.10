{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ST-GCN VIZULIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T23:59:08.601462Z",
     "iopub.status.busy": "2025-05-06T23:59:08.601026Z",
     "iopub.status.idle": "2025-05-06T23:59:08.680272Z",
     "shell.execute_reply": "2025-05-06T23:59:08.679551Z",
     "shell.execute_reply.started": "2025-05-06T23:59:08.601438Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model from /kaggle/working/best_stgcn_model_state_focused.pth\n",
      "Input dimension: 102 (features_per_joint=6, NUM_JOINTS=17)\n",
      "Loaded scaler from /kaggle/working/best_stgcn_scaler_focused.joblib\n",
      "Model loaded successfully\n",
      "Processing skeleton file: /kaggle/input/fine-fs-dataset/skeleton/skeleton/1.npz\n",
      "Loaded skeleton with shape: (4450, 17, 3)\n",
      "Truncated sequence from 4450 to 1000 frames\n",
      "Loading annotation file: /kaggle/input/fine-fs-dataset/annotation/annotation/1.json\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running inference...\n",
      "\n",
      "Prediction Results:\n",
      "+----------------+--------------+-------------------+--------------+\n",
      "| Component      |   True Score |   Predicted Score |   Difference |\n",
      "+================+==============+===================+==============+\n",
      "| skating_skills |         6.46 |             -0.61 |        -7.07 |\n",
      "+----------------+--------------+-------------------+--------------+\n",
      "| transitions    |         6.54 |             -0.63 |        -7.17 |\n",
      "+----------------+--------------+-------------------+--------------+\n",
      "| performance    |         6.64 |             -0.62 |        -7.26 |\n",
      "+----------------+--------------+-------------------+--------------+\n",
      "| composition    |         6.64 |             -0.64 |        -7.28 |\n",
      "+----------------+--------------+-------------------+--------------+\n",
      "| interpretation |         6.75 |             -0.52 |        -7.27 |\n",
      "+----------------+--------------+-------------------+--------------+\n",
      "\n",
      "Mean Squared Error: 51.9739\n",
      "Mean Absolute Error: 7.2089\n",
      "\n",
      "Comparison with Best Model Test Set Metrics:\n",
      "+----------------+--------------+------------------+--------------+------------------+\n",
      "| Component      |   Sample MSE |   Best Model MSE |   Sample MAE |   Best Model MAE |\n",
      "+================+==============+==================+==============+==================+\n",
      "| skating_skills |      49.9819 |          79.5908 |       7.0698 |           7.2547 |\n",
      "+----------------+--------------+------------------+--------------+------------------+\n",
      "| transitions    |      51.3823 |          78.1765 |       7.1681 |           7.1162 |\n",
      "+----------------+--------------+------------------+--------------+------------------+\n",
      "| performance    |      52.7058 |          79.0042 |       7.2599 |           7.1892 |\n",
      "+----------------+--------------+------------------+--------------+------------------+\n",
      "| composition    |      52.9307 |          85.2756 |       7.2753 |           7.5109 |\n",
      "+----------------+--------------+------------------+--------------+------------------+\n",
      "| interpretation |      52.8691 |          77.4202 |       7.2711 |           7.2022 |\n",
      "+----------------+--------------+------------------+--------------+------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3962972331.py:313: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import joblib\n",
    "\n",
    "\n",
    "BEST_MODEL_PATH = \"/kaggle/working/best_stgcn_model_state_focused.pth\"\n",
    "BEST_SCALER_PATH = \"/kaggle/working/best_stgcn_scaler_focused.joblib\"\n",
    "COMPONENTS_LIST = [\"skating_skills\", \"transitions\", \"performance\", \"composition\", \"interpretation\"]\n",
    "OUTPUT_DIM = len(COMPONENTS_LIST)\n",
    "\n",
    "\n",
    "SEQ_LEN = 1000\n",
    "INCLUDE_VELOCITIES = True\n",
    "INCLUDE_ACCELERATIONS = False\n",
    "POSITION_ENCODING = \"absolute\"\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "NUM_JOINTS = 17  \n",
    "\n",
    "adjacency_matrix = torch.zeros((NUM_JOINTS, NUM_JOINTS))\n",
    "\n",
    "for i in range(NUM_JOINTS):\n",
    "    adjacency_matrix[i, i] = 1\n",
    "\n",
    "for i in range(NUM_JOINTS-1):\n",
    "    adjacency_matrix[i, i+1] = 1\n",
    "    adjacency_matrix[i+1, i] = 1\n",
    "\n",
    "\n",
    "adjacency_matrix = adjacency_matrix.to(DEVICE)\n",
    "\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, adjacency_matrix):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.adj = adjacency_matrix\n",
    "\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / np.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def normalize_adjacency(self, adj):\n",
    "        adj = adj + torch.eye(adj.size(0), device=adj.device)\n",
    "        degree = torch.sum(adj, dim=1)\n",
    "        degree_inv_sqrt = torch.pow(degree, -0.5)\n",
    "        degree_inv_sqrt[torch.isinf(degree_inv_sqrt)] = 0.\n",
    "        degree_matrix_inv_sqrt = torch.diag(degree_inv_sqrt)\n",
    "        adj_normalized = torch.matmul(torch.matmul(degree_matrix_inv_sqrt, adj), degree_matrix_inv_sqrt)\n",
    "        return adj_normalized\n",
    "\n",
    "    def forward(self, input):\n",
    "        support = torch.matmul(input.transpose(1, 2), self.weight).transpose(1, 2)\n",
    "        output = torch.matmul(support, self.adj)\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias.view(1, -1, 1)\n",
    "        return output\n",
    "\n",
    "class STGCNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, spatial_out_channels, temporal_out_channels,\n",
    "                 temporal_kernel_size, adjacency_matrix, use_residual=True, dropout=0.0):\n",
    "        super(STGCNBlock, self).__init__()\n",
    "\n",
    "        self.use_residual = use_residual\n",
    "        self.spatial_out_channels = spatial_out_channels\n",
    "        self.temporal_out_channels = temporal_out_channels\n",
    "\n",
    "        normalized_adj = self.normalize_adjacency(adjacency_matrix)\n",
    "        self.gcn = GraphConvolution(in_channels, spatial_out_channels, normalized_adj)\n",
    "\n",
    "        padding = temporal_kernel_size // 2\n",
    "        self.tcn = nn.Conv1d(spatial_out_channels, temporal_out_channels,\n",
    "                             temporal_kernel_size, padding=padding)\n",
    "\n",
    "        self.bn_spatial = nn.BatchNorm1d(spatial_out_channels)\n",
    "        self.bn_temporal = nn.BatchNorm1d(temporal_out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        if self.use_residual and in_channels != temporal_out_channels:\n",
    "            self.residual_conv = nn.Conv2d(in_channels, temporal_out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.residual_conv = None\n",
    "\n",
    "    def normalize_adjacency(self, adj):\n",
    "        adj = adj + torch.eye(adj.size(0), device=adj.device)\n",
    "        degree = torch.sum(adj, dim=1)\n",
    "        degree_inv_sqrt = torch.pow(degree, -0.5)\n",
    "        degree_inv_sqrt[torch.isinf(degree_inv_sqrt)] = 0.\n",
    "        degree_matrix_inv_sqrt = torch.diag(degree_inv_sqrt)\n",
    "        adj_normalized = torch.matmul(torch.matmul(degree_matrix_inv_sqrt, adj), degree_matrix_inv_sqrt)\n",
    "        return adj_normalized\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        N, C, V, T = x.size()\n",
    "        x_gc = x.permute(0, 3, 1, 2).contiguous().view(N * T, C, V)\n",
    "        x_gc = self.gcn(x_gc)\n",
    "        x_gc = self.bn_spatial(x_gc)\n",
    "        x_gc = self.relu(x_gc)\n",
    "        x_gc = self.dropout(x_gc)\n",
    "\n",
    "        x_tcn = x_gc.view(N, T, self.spatial_out_channels, V).permute(0, 2, 3, 1).contiguous()\n",
    "        x_tcn = x_tcn.view(N * V, self.spatial_out_channels, T)\n",
    "\n",
    "        x_tcn = self.tcn(x_tcn)\n",
    "        x_tcn = self.bn_temporal(x_tcn)\n",
    "        x_tcn = self.relu(x_tcn)\n",
    "        x_tcn = self.dropout(x_tcn)\n",
    "\n",
    "        x_out = x_tcn.view(N, V, self.temporal_out_channels, T).permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        if self.use_residual:\n",
    "            if self.residual_conv is not None:\n",
    "                residual = self.residual_conv(residual)\n",
    "            x_out = x_out + residual\n",
    "\n",
    "        return x_out\n",
    "\n",
    "class STGCNRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, num_joints, seq_len, output_dim, adjacency_matrix, stgcn_params, fc_dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_joints = num_joints\n",
    "        self.seq_len = seq_len\n",
    "        self.adj = adjacency_matrix\n",
    "\n",
    "        initial_channels = input_dim // num_joints\n",
    "        if input_dim % num_joints != 0:\n",
    "            raise ValueError(f\"Input dimension ({input_dim}) must be divisible by number of joints ({num_joints}) for STGCN.\")\n",
    "\n",
    "        \n",
    "        num_blocks = stgcn_params.get('NUM_STGCN_BLOCKS', 2)\n",
    "        spatial_channels_list = stgcn_params.get('STGCN_SPATIAL_CHANNELS', 64)\n",
    "        temporal_channels_list = stgcn_params.get('STGCN_TEMPORAL_CHANNELS', 256)\n",
    "        temporal_kernel_size = stgcn_params.get('STGCN_KERNEL_SIZE', 9)\n",
    "        use_residual = stgcn_params.get('STGCN_USE_RESIDUAL', True)\n",
    "        block_dropout = stgcn_params.get('STGCN_BLOCK_DROPOUT', 0.3)\n",
    "\n",
    "        \n",
    "        if not isinstance(spatial_channels_list, (list, tuple)):\n",
    "            spatial_channels = [spatial_channels_list] * num_blocks\n",
    "        else:\n",
    "            spatial_channels = list(spatial_channels_list)\n",
    "        \n",
    "        if not isinstance(temporal_channels_list, (list, tuple)):\n",
    "            temporal_channels = [temporal_channels_list] * num_blocks\n",
    "        else:\n",
    "            temporal_channels = list(temporal_channels_list)\n",
    "\n",
    "     \n",
    "        self.stgcn_blocks = nn.ModuleList()\n",
    "        in_c = initial_channels\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            spatial_out_c = spatial_channels[i]\n",
    "            temporal_out_c = temporal_channels[i]\n",
    "            self.stgcn_blocks.append(\n",
    "                STGCNBlock(in_c, spatial_out_c, temporal_out_c,\n",
    "                           temporal_kernel_size, self.adj, use_residual, block_dropout)\n",
    "            )\n",
    "            in_c = temporal_out_c\n",
    "\n",
    "     \n",
    "        final_temporal_channels = temporal_channels[-1]\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        pooled_fc_input_dim = final_temporal_channels\n",
    "\n",
    "   \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(pooled_fc_input_dim, pooled_fc_input_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(fc_dropout),\n",
    "            nn.Linear(pooled_fc_input_dim // 2, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, total_features_flat = x.size()\n",
    "        total_features_per_joint = total_features_flat // self.num_joints\n",
    "        if total_features_flat % self.num_joints != 0:\n",
    "            raise ValueError(f\"Input feature dimension {total_features_flat} not divisible by {self.num_joints} joints.\")\n",
    "\n",
    "        x = x.view(N, T, self.num_joints, total_features_per_joint)\n",
    "        x = x.permute(0, 3, 2, 1).contiguous()\n",
    "\n",
    "        for block in self.stgcn_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        pooled = self.pool(x)\n",
    "        pooled = pooled.view(pooled.size(0), -1)\n",
    "\n",
    "        return self.fc(pooled)\n",
    "\n",
    "def load_and_process_skeleton(npz_file_path, seq_len=SEQ_LEN, include_velocities=INCLUDE_VELOCITIES, \n",
    "                             include_accelerations=INCLUDE_ACCELERATIONS, \n",
    "                             position_encoding=POSITION_ENCODING, scaler=None):\n",
    "    \"\"\"Load and process a single skeleton .npz file for inference\"\"\"\n",
    "    try:\n",
    "        data = np.load(npz_file_path)['reconstruction']  \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {npz_file_path}: {e}\")\n",
    "        return None\n",
    "        \n",
    "    T, J, C = data.shape\n",
    "    print(f\"Loaded skeleton with shape: {data.shape}\")\n",
    "    if T == 0 or C != 3:\n",
    "        print(f\"Invalid data shape: {data.shape}\")\n",
    "        return None\n",
    "        \n",
    "    data = data.astype(np.float32)  \n",
    "    \n",
    "\n",
    "    x_processed = data\n",
    "    if position_encoding == 'relative_root':\n",
    "        root_joint_index = 0\n",
    "        root_coords = data[:, root_joint_index:root_joint_index+1, :]\n",
    "        x_processed = data - root_coords\n",
    "    \n",
    "\n",
    "    x_flat_spatial = x_processed.reshape(T, J * C)\n",
    "    \n",
    "\n",
    "    feature_list = [x_flat_spatial]\n",
    "    \n",
    "    if include_velocities:\n",
    "        if T > 1:\n",
    "            velocities = np.diff(x_flat_spatial, axis=0, prepend=x_flat_spatial[0:1, :] * 0)\n",
    "        else:\n",
    "            velocities = np.zeros_like(x_flat_spatial, dtype=np.float32)\n",
    "        feature_list.append(velocities)\n",
    "    \n",
    "    if include_accelerations:\n",
    "        temp_velocities = np.diff(x_flat_spatial, axis=0, prepend=x_flat_spatial[0:1, :] * 0)\n",
    "        if T > 2:\n",
    "            accelerations = np.diff(temp_velocities, axis=0, prepend=temp_velocities[0:1, :] * 0)\n",
    "        else:\n",
    "            accelerations = np.zeros_like(x_flat_spatial, dtype=np.float32)\n",
    "        feature_list.append(accelerations)\n",
    "    \n",
    "\n",
    "    x_combined_flat = np.concatenate(feature_list, axis=1)\n",
    "    \n",
    "\n",
    "    if scaler is not None and hasattr(scaler, 'mean_'):\n",
    "        try:\n",
    "            x_combined_flat = scaler.transform(x_combined_flat)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during scaling: {e}\")\n",
    "    \n",
    "\n",
    "    if T < seq_len:\n",
    "        pad_shape = (seq_len - T, x_combined_flat.shape[1])\n",
    "        pad = np.zeros(pad_shape, dtype=np.float32)\n",
    "        x_final = np.vstack([x_combined_flat, pad])\n",
    "        print(f\"Padded sequence from {T} to {seq_len} frames\")\n",
    "    else:\n",
    "        x_final = x_combined_flat[:seq_len]\n",
    "        print(f\"Truncated sequence from {T} to {seq_len} frames\")\n",
    "    \n",
    "    return x_final\n",
    "\n",
    "def load_annotation(json_file_path):\n",
    "    \"\"\"Load component scores from a JSON annotation file\"\"\"\n",
    "    try:\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            annotation = json.load(f)\n",
    "        \n",
    "       \n",
    "        components = {}\n",
    "        for comp in COMPONENTS_LIST:\n",
    "            if comp in annotation['program_component']:\n",
    "                print()\n",
    "                components[comp] = annotation['program_component'][comp]['score_of_pannel']\n",
    "            else:\n",
    "                print(f\"Warning: Component {comp} not found in annotation\")\n",
    "                components[comp] = 0.0\n",
    "                \n",
    "        return components\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load annotation from {json_file_path}: {e}\")\n",
    "        return {comp: 0.0 for comp in COMPONENTS_LIST}\n",
    "\n",
    "def main():\n",
    "\n",
    "    npz_file = \"/kaggle/input/fine-fs-dataset/skeleton/skeleton/1.npz\"  \n",
    "    json_file = \"/kaggle/input/fine-fs-dataset/annotation/annotation/1.json\"  \n",
    "    \n",
    "    print(f\"Loading model from {BEST_MODEL_PATH}\")\n",
    "    \n",
    "    try:\n",
    " \n",
    "        checkpoint = torch.load(BEST_MODEL_PATH, map_location=DEVICE)\n",
    "        model_state = checkpoint.get('model_state_dict', checkpoint)  \n",
    "        \n",
    "        \n",
    "        stgcn_params = {\n",
    "            'NUM_STGCN_BLOCKS': 2,\n",
    "            'STGCN_SPATIAL_CHANNELS': 64,\n",
    "            'STGCN_TEMPORAL_CHANNELS': 256,\n",
    "            'STGCN_KERNEL_SIZE': 9,\n",
    "            'STGCN_USE_RESIDUAL': True,\n",
    "            'STGCN_BLOCK_DROPOUT': 0.0 \n",
    "        }\n",
    "        fc_dropout = 0.0  \n",
    "        \n",
    "       \n",
    "        features_per_joint = 3  # \n",
    "        if INCLUDE_VELOCITIES:\n",
    "            features_per_joint += 3  \n",
    "        if INCLUDE_ACCELERATIONS:\n",
    "            features_per_joint += 3  \n",
    "            \n",
    "        input_dim = NUM_JOINTS * features_per_joint\n",
    "        print(f\"Input dimension: {input_dim} (features_per_joint={features_per_joint}, NUM_JOINTS={NUM_JOINTS})\")\n",
    "        \n",
    "       \n",
    "        scaler = None\n",
    "        try:\n",
    "            scaler = joblib.load(BEST_SCALER_PATH)\n",
    "            print(f\"Loaded scaler from {BEST_SCALER_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load scaler: {e}, proceeding without scaling\")\n",
    "        \n",
    "        \n",
    "        model = STGCNRegressor(\n",
    "            input_dim=input_dim, \n",
    "            num_joints=NUM_JOINTS, \n",
    "            seq_len=SEQ_LEN, \n",
    "            output_dim=OUTPUT_DIM,\n",
    "            adjacency_matrix=adjacency_matrix,\n",
    "            stgcn_params=stgcn_params,\n",
    "            fc_dropout=fc_dropout\n",
    "        )\n",
    "        \n",
    "        \n",
    "        model.load_state_dict(model_state)\n",
    "        model.to(DEVICE)\n",
    "        model.eval()\n",
    "        \n",
    "        print(\"Model loaded successfully\")\n",
    "        \n",
    "        \n",
    "        print(f\"Processing skeleton file: {npz_file}\")\n",
    "        features = load_and_process_skeleton(\n",
    "            npz_file,\n",
    "            seq_len=SEQ_LEN,\n",
    "            include_velocities=INCLUDE_VELOCITIES,\n",
    "            include_accelerations=INCLUDE_ACCELERATIONS,\n",
    "            position_encoding=POSITION_ENCODING,\n",
    "            scaler=scaler\n",
    "        )\n",
    "        \n",
    "        if features is None:\n",
    "            print(\"Failed to process skeleton data\")\n",
    "            return\n",
    "            \n",
    "        \n",
    "        print(f\"Loading annotation file: {json_file}\")\n",
    "        true_components = load_annotation(json_file)\n",
    "        \n",
    "        \n",
    "        features_tensor = torch.tensor(features).unsqueeze(0).float().to(DEVICE)  # Add batch dimension\n",
    "        \n",
    "        \n",
    "        print(\"Running inference...\")\n",
    "        with torch.no_grad():\n",
    "            predicted_scores = model(features_tensor)\n",
    "            predicted_scores = predicted_scores.cpu().numpy()[0]  \n",
    "        \n",
    "        \n",
    "        table_data = []\n",
    "        for i, comp_name in enumerate(COMPONENTS_LIST):\n",
    "            true_score = true_components.get(comp_name, \"N/A\")\n",
    "            pred_score = predicted_scores[i]\n",
    "            true_score = float(true_score)\n",
    "            pred_score = float(pred_score)\n",
    "            if isinstance(true_score, (int, float)) and isinstance(pred_score, (int, float)):\n",
    "                diff = pred_score - true_score\n",
    "                table_data.append([comp_name, f\"{true_score:.2f}\", f\"{pred_score:.2f}\", f\"{diff:.2f}\"])\n",
    "            else:\n",
    "                table_data.append([comp_name, true_score, f\"{pred_score:.2f}\", \"N/A\"])\n",
    "        \n",
    "        print(\"\\nPrediction Results:\")\n",
    "        headers = [\"Component\", \"True Score\", \"Predicted Score\", \"Difference\"]\n",
    "        print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
    "        \n",
    "        \n",
    "        if all(comp in true_components for comp in COMPONENTS_LIST):\n",
    "            true_array = np.array([true_components[comp] for comp in COMPONENTS_LIST])\n",
    "            mse = np.mean((true_array - predicted_scores)**2)\n",
    "            mae = np.mean(np.abs(true_array - predicted_scores))\n",
    "            print(f\"\\nMean Squared Error: {mse:.4f}\")\n",
    "            print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "            \n",
    "            \n",
    "            print(\"\\nComparison with Best Model Test Set Metrics:\")\n",
    "            config_metrics = {\n",
    "                \"skating_skills\": {\"MSE\": 79.590846, \"MAE\": 7.254685, \"R2\": -0.249512},\n",
    "                \"transitions\": {\"MSE\": 78.176524, \"MAE\": 7.116168, \"R2\": -0.313296},\n",
    "                \"performance\": {\"MSE\": 79.004186, \"MAE\": 7.189218, \"R2\": -0.305964},\n",
    "                \"composition\": {\"MSE\": 85.275591, \"MAE\": 7.510861, \"R2\": -0.325712},\n",
    "                \"interpretation\": {\"MSE\": 77.420197, \"MAE\": 7.202172, \"R2\": -0.311066}\n",
    "            }\n",
    "            \n",
    "            comparison_table = []\n",
    "            for comp in COMPONENTS_LIST:\n",
    "                true_val = true_components.get(comp, 0)\n",
    "                pred_val = predicted_scores[COMPONENTS_LIST.index(comp)]\n",
    "                comp_mse = (true_val - pred_val)**2\n",
    "                comp_mae = abs(true_val - pred_val)\n",
    "                comparison_table.append([\n",
    "                    comp, \n",
    "                    f\"{comp_mse:.4f}\", \n",
    "                    f\"{config_metrics[comp]['MSE']:.4f}\",\n",
    "                    f\"{comp_mae:.4f}\", \n",
    "                    f\"{config_metrics[comp]['MAE']:.4f}\"\n",
    "                ])\n",
    "            \n",
    "            comp_headers = [\"Component\", \"Sample MSE\", \"Best Model MSE\", \"Sample MAE\", \"Best Model MAE\"]\n",
    "            print(tabulate(comparison_table, headers=comp_headers, tablefmt=\"grid\"))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM VIZULIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T00:14:40.084798Z",
     "iopub.status.busy": "2025-05-07T00:14:40.084167Z",
     "iopub.status.idle": "2025-05-07T00:14:40.191682Z",
     "shell.execute_reply": "2025-05-07T00:14:40.190849Z",
     "shell.execute_reply.started": "2025-05-07T00:14:40.084774Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded scaler from /kaggle/input/fine-fs-dataset/best_lstm_scaler_focused.joblib\n",
      "Loaded skeleton data with shape: (4450, 17, 3)\n",
      "Loaded annotation data with scores: {'skating_skills': 6.46, 'transitions': 6.54, 'performance': 6.64, 'composition': 6.64, 'interpretation': 6.75}\n",
      "Loaded model from /kaggle/input/fine-fs-dataset/best_lstm_model_state_focused.pth\n",
      "\n",
      "--- Inference Results ---\n",
      "╒════╤════════════════╤══════════╤═════════════╤══════════════╕\n",
      "│    │ Component      │   Actual │   Predicted │   Difference │\n",
      "╞════╪════════════════╪══════════╪═════════════╪══════════════╡\n",
      "│  0 │ skating_skills │     6.46 │        4.88 │         1.58 │\n",
      "├────┼────────────────┼──────────┼─────────────┼──────────────┤\n",
      "│  1 │ transitions    │     6.54 │        4.89 │         1.65 │\n",
      "├────┼────────────────┼──────────┼─────────────┼──────────────┤\n",
      "│  2 │ performance    │     6.64 │        4.83 │         1.81 │\n",
      "├────┼────────────────┼──────────┼─────────────┼──────────────┤\n",
      "│  3 │ composition    │     6.64 │        4.86 │         1.78 │\n",
      "├────┼────────────────┼──────────┼─────────────┼──────────────┤\n",
      "│  4 │ interpretation │     6.75 │        4.89 │         1.86 │\n",
      "╘════╧════════════════╧══════════╧═════════════╧══════════════╛\n",
      "\n",
      "Overall Mean Absolute Error: 1.7356\n",
      "Overall Root Mean Squared Error: 1.7386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/2294375543.py:144: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tabulate import tabulate  \n",
    "\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, lstm_dropout, fc_dropout, pooling_strategy='mean'):\n",
    "        super().__init__()\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,\n",
    "                            batch_first=True, bidirectional=True,\n",
    "                            dropout=lstm_dropout if num_layers > 1 else 0)\n",
    "\n",
    "        fc_input_dim = hidden_dim * 2\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(fc_input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(fc_dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(fc_dropout),\n",
    "            nn.Linear(hidden_dim // 2, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs, (hn, cn) = self.lstm(x)\n",
    "\n",
    "        if self.pooling_strategy == 'mean':\n",
    "            pooled = torch.mean(outs, dim=1)\n",
    "        elif self.pooling_strategy == 'last':\n",
    "            pooled = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            pooled = torch.mean(outs, dim=1)\n",
    "\n",
    "        return self.fc(pooled)\n",
    "\n",
    "\n",
    "def inference_single(skeleton_npz_path, annotation_json_path, model_path, scaler_path, device='cuda'):\n",
    "   \n",
    "    \n",
    "    if device == 'cuda' and not torch.cuda.is_available():\n",
    "        print(\"CUDA not available. Using CPU instead.\")\n",
    "        device = 'cpu'\n",
    "    \n",
    "    device = torch.device(device)\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    \n",
    "    components_list = ['skating_skills', 'transitions', 'performance', 'composition', 'interpretation']\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        scaler = joblib.load(scaler_path)\n",
    "        print(f\"Loaded scaler from {scaler_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading scaler: {e}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    hidden_dim = 256  \n",
    "    num_layers = 2\n",
    "    lstm_dropout = 0.3\n",
    "    fc_dropout = 0.4\n",
    "    output_dim = len(components_list)\n",
    "    pooling_strategy = 'last'\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        skeleton_data = np.load(skeleton_npz_path)['reconstruction']\n",
    "        print(f\"Loaded skeleton data with shape: {skeleton_data.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading skeleton data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        with open(annotation_json_path, 'r') as f:\n",
    "            annotation_data = json.load(f)\n",
    "        \n",
    "        \n",
    "        actual_scores = {}\n",
    "        for component in components_list:\n",
    "            \n",
    "            if component in annotation_data['program_component']:\n",
    "                actual_scores[component] = annotation_data['program_component'][component]['score_of_pannel']\n",
    "            else:\n",
    "                print(f\"Warning: Component '{component}' not found in annotation data\")\n",
    "                actual_scores[component] = float('nan')\n",
    "        \n",
    "        print(f\"Loaded annotation data with scores: {actual_scores}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading annotation data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    T, J, C = skeleton_data.shape\n",
    "    \n",
    "    \n",
    "    x_coords = skeleton_data.reshape(T, J * C).astype(np.float32)\n",
    "    \n",
    "   \n",
    "    if T > 1:\n",
    "        velocities = np.diff(x_coords, axis=0)\n",
    "        velocities = np.vstack([np.zeros((1, J * C), dtype=np.float32), velocities])\n",
    "    else:\n",
    "        velocities = np.zeros_like(x_coords, dtype=np.float32)\n",
    "    \n",
    "    x_combined = np.concatenate([x_coords, velocities], axis=1)\n",
    "    \n",
    "    \n",
    "    if x_combined.shape[1] != scaler.mean_.shape[0]:\n",
    "        print(f\"Warning: Feature dimension mismatch. Data has {x_combined.shape[1]} features, scaler expects {scaler.mean_.shape[0]}.\")\n",
    "        return None\n",
    "    \n",
    "    x_scaled = scaler.transform(x_combined)\n",
    "    \n",
    "    \n",
    "    seq_len = 1000  \n",
    "    if T < seq_len:\n",
    "        pad_shape = (seq_len - T, x_scaled.shape[1])\n",
    "        pad = np.zeros(pad_shape, dtype=np.float32)\n",
    "        x_final = np.vstack([x_scaled, pad])\n",
    "    else:\n",
    "        x_final = x_scaled[:seq_len]\n",
    "    \n",
    "    \n",
    "    x_tensor = torch.from_numpy(x_final).unsqueeze(0).to(device)  \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        input_dim = x_tensor.shape[2]  \n",
    "        model = LSTMRegressor(input_dim, hidden_dim, num_layers, output_dim, \n",
    "                             lstm_dropout, fc_dropout, pooling_strategy).to(device)\n",
    "        \n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        print(f\"Loaded model from {model_path}\")\n",
    "        model.eval()\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up model: {e}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        normalized_pred = model(x_tensor)\n",
    "        normalized_pred = normalized_pred.cpu().numpy()[0]  \n",
    "    \n",
    "   \n",
    "    target_means = np.array([5.0, 5.0, 5.0, 5.0, 5.0])  \n",
    "    target_stds = np.array([1.0, 1.0, 1.0, 1.0, 1.0])   \n",
    "    \n",
    "    \n",
    "    predictions = normalized_pred * target_stds + target_means\n",
    "    \n",
    "    \n",
    "    results = {\n",
    "        'Component': components_list,\n",
    "        'Actual': [actual_scores.get(comp, float('nan')) for comp in components_list],\n",
    "        'Predicted': predictions.tolist(),\n",
    "        'Difference': [(actual_scores.get(comp, float('nan')) - pred) for comp, pred in zip(components_list, predictions)]\n",
    "    }\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # File paths\n",
    "    skeleton_npz_path = '/kaggle/input/fine-fs-dataset/skeleton/skeleton/1.npz'  # Replace with actual file path\n",
    "    annotation_json_path = '/kaggle/input/fine-fs-dataset/annotation/annotation/1.json'  # Replace with actual file path\n",
    "    model_path = '/kaggle/input/fine-fs-dataset/best_lstm_model_state_focused.pth'\n",
    "    scaler_path = '/kaggle/input/fine-fs-dataset/best_lstm_scaler_focused.joblib'\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(skeleton_npz_path):\n",
    "        skeleton_npz_path = input(\"Enter path to skeleton NPZ file: \")\n",
    "    \n",
    "    if not os.path.exists(annotation_json_path):\n",
    "        annotation_json_path = input(\"Enter path to annotation JSON file: \")\n",
    "    \n",
    "    \n",
    "    results_df = inference_single(skeleton_npz_path, annotation_json_path, model_path, scaler_path)\n",
    "    \n",
    "    if results_df is not None:\n",
    "        \n",
    "        print(\"\\n--- Inference Results ---\")\n",
    "        print(tabulate(results_df, headers='keys', tablefmt='fancy_grid', floatfmt=\".2f\"))\n",
    "        \n",
    "        \n",
    "        mae = np.mean(np.abs(results_df['Difference'].values))\n",
    "        rmse = np.sqrt(np.mean(np.square(results_df['Difference'].values)))\n",
    "        \n",
    "        print(f\"\\nOverall Mean Absolute Error: {mae:.4f}\")\n",
    "        print(f\"Overall Root Mean Squared Error: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7325341,
     "sourceId": 11709113,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
